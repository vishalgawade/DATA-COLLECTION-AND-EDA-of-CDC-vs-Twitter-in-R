{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name-Vishal Gawade\n",
    "UbId-vgawade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Attaching package: ‘twitteR’\n",
      "\n",
      "The following object is masked from ‘package:plyr’:\n",
      "\n",
      "    id\n",
      "\n",
      "Google's Terms of Service: https://cloud.google.com/maps-platform/terms/.\n",
      "Please cite ggmap if you use it! See citation(\"ggmap\") for details.\n",
      "Loading required package: sp\n",
      "Checking rgeos availability: FALSE\n",
      " \tNote: when rgeos is not available, polygon geometry \tcomputations in maptools depend on gpclib,\n",
      " \twhich has a restricted licence. It is disabled by default;\n",
      " \tto enable gpclib, type gpclibPermit()\n",
      "\n",
      "Attaching package: ‘maps’\n",
      "\n",
      "The following object is masked from ‘package:plyr’:\n",
      "\n",
      "    ozone\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# install.packages(\"twitteR\")\n",
    "# Installing\n",
    "# install.packages(\"readr\")\n",
    "# install.packages(\"sp\")\n",
    "# install.packages(\"maps\")\n",
    "# install.packages(\"maptools\")\n",
    "# Loading\n",
    "library(\"readr\")\n",
    "library(stringr)\n",
    "library(usmap)\n",
    "# install.packages(\"plyr\")\n",
    "library(plyr)\n",
    "library(ggplot2)\n",
    "library(twitteR)\n",
    "\n",
    "library(\"ggmap\")\n",
    "library(\"maptools\")\n",
    "library(maps)\n",
    "library(sp)\n",
    "library(maptools)\n",
    "library(ggmap)\n",
    "register_google(key = \"key here\") \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't run following code again-it is for preprocess and data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #twitter api oauth\n",
    "# api_key <- \"key here\"\n",
    "# api_secret <- \"key here\"\n",
    "# access_token <- \"key here\"\n",
    "# access_token_secret <- \"key here\"\n",
    "# setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)\n",
    "\n",
    "\n",
    "# #get an approximate US tweets\n",
    "# us <- \"40.48,-97.41,2000mi\"\n",
    "\n",
    "# #call api\n",
    "# tweets <- searchTwitter('Influenza OR flu OR #fluview OR #flu OR #influenza',since=\"2019-02-02\" , n=10000, lang=\"en\", geocode = us)\n",
    "\n",
    "# dataFrames <- twListToDF(tweets)\n",
    "# # head(dataFrames)\n",
    "\n",
    "# screenNames <- dataFrames$screenName\n",
    "\n",
    "# users <- lookupUsers(screenNames)\n",
    "\n",
    "# # convert the screenNames to dataFrames\n",
    "# userFrames <- twListToDF(users)\n",
    "\n",
    "# # head(userFrames)\n",
    "# #remove null user locations\n",
    "# naUsers <- !is.na(userFrames$location)\n",
    "\n",
    "# # get the location from naUsers \n",
    "# locations <- userFrames$location[naUsers]\n",
    "# head(locations)\n",
    "# write_csv(dataFrames, \"raw_tweets.csv\",append = TRUE)\n",
    "\n",
    "# # getting whole address from geocode api \n",
    "# #ldply which return a dataframe\n",
    "# locationsNew <- ldply(locations, function(loc) geocode(loc,output = \"more\", source = \"google\"))\n",
    "# head(locationsNew)\n",
    "\n",
    "# # get states from Locations\n",
    "# states <- subset(locationsNew, grepl(\", usa\", locationsNew$address)==TRUE) \n",
    "# head(states)                            \n",
    "# write_csv(states, \"statesList.csv\",append = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #twitter api oauth\n",
    "# api_key <- \"key here\"\n",
    "# api_secret <- \"key here\"\n",
    "# access_token <- \"key here\"\n",
    "# access_token_secret <- \"key here\"\n",
    "# setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)\n",
    "\n",
    "\n",
    "# #get an approximate US tweets\n",
    "# us <- \"40.48,-97.41,2000mi\"\n",
    "\n",
    "# #call api\n",
    "# tweets <- searchTwitter('flu',since=\"2019-01-01\", until=\"2019-02-28\", n=1000, lang=\"en\", geocode = us)\n",
    "\n",
    "# dataFrames <- twListToDF(tweets)\n",
    "# # head(dataFrames)\n",
    "\n",
    "# screenNames <- dataFrames$screenName\n",
    "\n",
    "# users <- lookupUsers(screenNames)\n",
    "\n",
    "# # convert the screenNames to dataFrames\n",
    "# userFrames <- twListToDF(users)\n",
    "\n",
    "# # head(userFrames)\n",
    "# #remove null user locations\n",
    "# naUsers <- !is.na(userFrames$location)\n",
    "\n",
    "# # get the location from naUsers \n",
    "# locations <- userFrames$location[naUsers]\n",
    "# head(locations)\n",
    "# write_csv(dataFrames, \"raw_tweets_flu_keyword.csv\",append = TRUE)\n",
    "\n",
    "# # getting whole address from geocode api \n",
    "# #ldply which return a dataframe\n",
    "# locationsNew <- ldply(locations, function(loc) geocode(loc,output = \"more\", source = \"google\"))\n",
    "# head(locationsNew)\n",
    "\n",
    "# # get states from Locations\n",
    "# states <- subset(locationsNew, grepl(\", usa\", locationsNew$address)==TRUE) \n",
    "# head(states)                            \n",
    "# write_csv(states, \"statesList_flu_keyword.csv\",append = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #twitter api oauth\n",
    "# api_key <- \"key here\"\n",
    "# api_secret <- \"key here\"\n",
    "# access_token <- \"key here\"\n",
    "# access_token_secret <- \"key here\"\n",
    "# setup_twitter_oauth(api_key, api_secret, access_token, access_token_secret)\n",
    "\n",
    "\n",
    "# #get an approximate US tweets\n",
    "# us <- \"40.48,-97.41,2000mi\"\n",
    "\n",
    "# #call api\n",
    "# tweets <- searchTwitter('Sneezing',since=\"2019-01-01\", until=\"2019-02-28\", n=1000, lang=\"en\", geocode = us)\n",
    "\n",
    "# dataFrames <- twListToDF(tweets)\n",
    "# # head(dataFrames)\n",
    "\n",
    "# screenNames <- dataFrames$screenName\n",
    "\n",
    "# users <- lookupUsers(screenNames)\n",
    "\n",
    "# # convert the screenNames to dataFrames\n",
    "# userFrames <- twListToDF(users)\n",
    "\n",
    "# # head(userFrames)\n",
    "# #remove null user locations\n",
    "# naUsers <- !is.na(userFrames$location)\n",
    "\n",
    "# # get the location from naUsers \n",
    "# locations <- userFrames$location[naUsers]\n",
    "# head(locations)\n",
    "# write_csv(dataFrames, \"raw_tweets_Sneezing_keyword.csv\",append = TRUE)\n",
    "\n",
    "# # getting whole address from geocode api \n",
    "# #ldply which return a dataframe\n",
    "# locationsNew <- ldply(locations, function(loc) geocode(loc,output = \"more\", source = \"google\"))\n",
    "# head(locationsNew)\n",
    "\n",
    "# # get states from Locations\n",
    "# states <- subset(locationsNew, grepl(\", usa\", locationsNew$address)==TRUE) \n",
    "# head(states)                            \n",
    "# write_csv(states, \"statesList_Sneezing_keyword.csv\",append = TRUE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get state names from state abrrevation\n",
    "getStateName <- function(abbrv){\n",
    "  ab    <- tolower(c(\"AL\",\n",
    "             \"AK\", \"AZ\", \"KS\", \"UT\", \"CO\", \"CT\",\n",
    "             \"DE\", \"FL\", \"GA\", \"HI\", \"ID\", \"IL\",\n",
    "             \"IN\", \"IA\", \"AR\", \"KY\", \"LA\", \"ME\",\n",
    "             \"MD\", \"MA\", \"MI\", \"MN\", \"MS\", \"MO\",\n",
    "             \"MT\", \"NE\", \"NV\", \"NH\", \"NJ\", \"NM\",\n",
    "             \"NY\", \"NC\", \"ND\", \"OH\", \"OK\", \"OR\",\n",
    "             \"PA\", \"RI\", \"SC\", \"SD\", \"TN\", \"TX\",\n",
    "             \"CA\", \"VT\", \"VA\", \"WA\", \"WV\", \"WI\",\n",
    "             \"WY\", \"DC\"))\n",
    "  st    <- c(\"Alabama\",\n",
    "             \"Alaska\", \"Arizona\", \"Kansas\",\n",
    "             \"Utah\", \"Colorado\", \"Connecticut\",\n",
    "             \"Delaware\", \"Florida\", \"Georgia\",\n",
    "             \"Hawaii\", \"Idaho\", \"Illinois\",\n",
    "             \"Indiana\", \"Iowa\", \"Arkansas\",\n",
    "             \"Kentucky\", \"Louisiana\", \"Maine\",\n",
    "             \"Maryland\", \"Massachusetts\", \"Michigan\",\n",
    "             \"Minnesota\", \"Mississippi\", \"Missouri\",\n",
    "             \"Montana\", \"Nebraska\", \"Nevada\",\n",
    "             \"New Hampshire\", \"New Jersey\", \"New Mexico\",\n",
    "             \"New York\", \"North Carolina\", \"North Dakota\",\n",
    "             \"Ohio\", \"Oklahoma\", \"Oregon\",\n",
    "             \"Pennsylvania\", \"Rhode Island\", \"South Carolina\",\n",
    "             \"South Dakota\", \"Tennessee\", \"Texas\",\n",
    "             \"California\", \"Vermont\", \"Virginia\",\n",
    "             \"Washington\", \"West Virginia\", \"Wisconsin\",\n",
    "             \"Wyoming\", \"District of Columbia\")\n",
    "  st[match(tolower(abbrv), ab)]\n",
    "}\n",
    "\n",
    "#get last n characters from string\n",
    "substrFromEnd <- function(x, n){\n",
    "  substr(x, nchar(x)-n+1, nchar(x))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [1] pennsylvania  florida       indiana       iowa          massachusetts\n",
      " [6] massachusetts illinois      massachusetts california    ohio         \n",
      "[11] new york      ohio          west virginia kentucky      florida      \n",
      "[16] illinois      new york      new york      michigan      colorado     \n",
      "[21] iowa          georgia       florida       alabama       missouri     \n",
      "[26] maryland      tennessee     florida       ohio          virginia     \n",
      "[31] nevada        new york      new york      new york      ohio         \n",
      "[36] ohio          colorado      illinois     \n",
      "51 Levels: alabama alaska arizona arkansas california colorado ... wyoming\n"
     ]
    }
   ],
   "source": [
    "#Not using lat_lon library\n",
    "# The single argument to this function, pointsDF, is a data.frame in which:\n",
    "#   - column 1 contains the longitude in degrees (negative in the US)\n",
    "#   - column 2 contains the latitude in degrees\n",
    "\n",
    "# latlong2state <- function(pointsDF) {\n",
    "#     # Prepare SpatialPolygons object with one SpatialPolygon\n",
    "#     # per state (plus DC, minus HI & AK)\n",
    "#     states <- map('state', fill=TRUE, col=\"transparent\", plot=FALSE)\n",
    "#     IDs <- sapply(strsplit(states$names, \":\"), function(x) x[1])\n",
    "#     states_sp <- map2SpatialPolygons(states, IDs=IDs,\n",
    "#                      proj4string=CRS(\"+proj=longlat +datum=WGS84\"))\n",
    "\n",
    "#     # Convert pointsDF to a SpatialPoints object \n",
    "#     pointsSP <- SpatialPoints(pointsDF, \n",
    "#                     proj4string=CRS(\"+proj=longlat +datum=WGS84\"))\n",
    "\n",
    "#     # Use 'over' to get _indices_ of the Polygons object containing each point \n",
    "#     indices <- over(pointsSP, states_sp)\n",
    "\n",
    "#     # Return the state names of the Polygons object containing each point\n",
    "#     stateNames <- sapply(states_sp@polygons, function(x) x@ID)\n",
    "#     stateNames[indices]\n",
    "# }\n",
    "# get states name in another csv for display.\n",
    "data <- read.csv(\"statesList.csv\", sep = \",\")\n",
    "# head(gsub('[[:digit:]]+', '', data$address))\n",
    "data$address=str_remove_all(data$address,\" \");\n",
    "data$address=str_remove_all(gsub('[[:digit:]]+', '', data$address),\",usa\");\n",
    "data$address=substrFromEnd(data$address,2);\n",
    "testPoints <- data.frame(x = tolower(getStateName(data$address)), y = data$lat)\n",
    "print(testPoints$x[!is.na(testPoints$x)][3:40]);\n",
    "# data <- read.csv(\"statesList.csv\", sep = \",\")\n",
    "# testPoints <- data.frame(x = data$lon, y = data$lat)\n",
    "# raw<-latlong2state(testPoints)\n",
    "# states <- raw[!is.na(raw)]\n",
    "states <- testPoints$x[!is.na(testPoints$x)]\n",
    "# head(states)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#state wise tweets count\n",
    "tweet_count <- as.data.frame(table(states))\n",
    "# print(tweet_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong>pdf:</strong> 2"
      ],
      "text/latex": [
       "\\textbf{pdf:} 2"
      ],
      "text/markdown": [
       "**pdf:** 2"
      ],
      "text/plain": [
       "pdf \n",
       "  2 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "png(\"cdc_vs_twitter.png\")\n",
    "us <- map_data(\"state\")                           \n",
    "us <- map_data(\"state\")\n",
    "tweet_count=data.frame(tweet_count)\n",
    "tweet_count$state<-tweet_count$states\n",
    "#ggtitle(\"2018-19 Influenza Season Week 7 ending Feb 16, 2019\")+\n",
    "twitter<-plot_usmap(data = tweet_count, values = \"Freq\", lines = \"black\") + \n",
    "  scale_fill_continuous(\n",
    "    low = \"green\", high = \"red\", name = \"Tweet Count\", label = scales::comma\n",
    "  ) + \n",
    "# labs(title = \"2018-19 Influenza Season Week 4 ending Jan 26, 2019\") + \n",
    "theme(legend.position = \"right\")\n",
    "\n",
    "# us <- map_data(\"state\")\n",
    "# StateDatabyWeekforMap_2018-19week40-9\n",
    "statedata <- read.csv(\"StateDataforMap_2018-19week4.csv\")\n",
    "statedata$activityval <- as.numeric(gsub(\"Level \",\"\",statedata$ACTIVITY.LEVEL))\n",
    "statedata$state <- tolower(statedata$STATENAME)\n",
    "#ggtitle(\"2018-19 Influenza Season Week 7 ending Feb 16, 2019\")+\n",
    "cdc<-plot_usmap(data = statedata, values = \"activityval\", lines = \"black\") + \n",
    "  scale_fill_continuous(\n",
    "    low = \"green\", high = \"red\", name = \"ILI Activity Level\", label = scales::comma\n",
    "  ) + \n",
    "# labs(title = \"2018-19 Influenza Season Week 4 ending Jan 26, 2019\") + \n",
    "theme(legend.position = \"right\")\n",
    "dev.off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/1249548/side-by-side-plots-with-ggplot2\n",
    "multiplot <- function(..., plotlist=NULL, cols) {\n",
    "    require(grid)\n",
    "\n",
    "    # Make a list from the ... arguments and plotlist\n",
    "    plots <- c(list(...), plotlist)\n",
    "\n",
    "    numPlots = length(plots)\n",
    "\n",
    "    # Make the panel\n",
    "    plotCols = cols                          # Number of columns of plots\n",
    "    plotRows = ceiling(numPlots/plotCols) # Number of rows needed, calculated from # of cols\n",
    "\n",
    "    # Set up the page\n",
    "    grid.newpage()\n",
    "    pushViewport(viewport(layout = grid.layout(plotRows, plotCols)))\n",
    "    vplayout <- function(x, y)\n",
    "        viewport(layout.pos.row = x, layout.pos.col = y)\n",
    "\n",
    "    # Make each plot, in the correct location\n",
    "    for (i in 1:numPlots) {\n",
    "        curRow = ceiling(i/plotCols)\n",
    "        curCol = (i-1) %% plotCols + 1\n",
    "        print(plots[[i]], vp = vplayout(curRow, curCol ))\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CDC VS Twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from CDC heat map of Influenza season,Influenza is mostly spread on south-east and central america.Our collected tweets heat map also shows it spread mostly on south-east and central america and also in California(west side state of america).California and Alaska are the states which mostly shows contradictory results for cdc and our map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: grid\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<strong>pdf:</strong> 2"
      ],
      "text/latex": [
       "\\textbf{pdf:} 2"
      ],
      "text/markdown": [
       "**pdf:** 2"
      ],
      "text/plain": [
       "pdf \n",
       "  2 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "##cdc vs Twitter\n",
    "# cat(\"\\n\\t\\tSide by side comparison of CDC 2018-19 Influenza Season VS Tweets made by different states\")\n",
    "png(\"cdc_vs_twitter.png\")\n",
    "multiplot(cdc, twitter, cols=2)\n",
    "dev.off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CDC VS Twitter(Sneezing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [1] new york             florida              georgia             \n",
      " [4] texas                district of columbia virginia            \n",
      " [7] alabama              tennessee            new york            \n",
      "[10] new york             kansas               california          \n",
      "[13] tennessee            missouri             tennessee           \n",
      "[16] new york             pennsylvania         indiana             \n",
      "[19] virginia             massachusetts        massachusetts       \n",
      "[22] washington           new york             maryland            \n",
      "[25] connecticut          massachusetts        florida             \n",
      "[28] district of columbia iowa                 iowa                \n",
      "[31] illinois             connecticut          new mexico          \n",
      "[34] indiana              indiana              south carolina      \n",
      "[37] new york             missouri            \n",
      "47 Levels: alabama arizona arkansas california colorado ... wyoming\n"
     ]
    }
   ],
   "source": [
    "data <- read.csv(\"statesList_Sneezing_keyword.csv\", sep = \",\")\n",
    "data$address=str_remove_all(data$address,\" \");\n",
    "data$address=str_remove_all(gsub('[[:digit:]]+', '', data$address),\",usa\");\n",
    "data$address=substrFromEnd(data$address,2);\n",
    "testPoints <- data.frame(x = tolower(getStateName(data$address)), y = data$lat)\n",
    "print(testPoints$x[!is.na(testPoints$x)][3:40]);\n",
    "# data <- read.csv(\"statesList.csv\", sep = \",\")\n",
    "# testPoints <- data.frame(x = data$lon, y = data$lat)\n",
    "# raw<-latlong2state(testPoints)\n",
    "# states <- raw[!is.na(raw)]\n",
    "states <- testPoints$x[!is.na(testPoints$x)]\n",
    "tweet_count <- as.data.frame(table(states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong>pdf:</strong> 2"
      ],
      "text/latex": [
       "\\textbf{pdf:} 2"
      ],
      "text/markdown": [
       "**pdf:** 2"
      ],
      "text/plain": [
       "pdf \n",
       "  2 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "png(\"cdc_vs_twitter_sneezing.png\")\n",
    "us <- map_data(\"state\")                           \n",
    "us <- map_data(\"state\")\n",
    "tweet_count=data.frame(tweet_count)\n",
    "tweet_count$state<-tweet_count$states\n",
    "#ggtitle(\"2018-19 Influenza Season Week 7 ending Feb 16, 2019\")+\n",
    "twitter<-plot_usmap(data = tweet_count, values = \"Freq\", lines = \"black\") + \n",
    "  scale_fill_continuous(\n",
    "    low = \"green\", high = \"red\", name = \"Tweet Count\", label = scales::comma\n",
    "  ) + \n",
    "# labs(title = \"2018-19 Influenza Season Week 4 ending Jan 26, 2019\") + \n",
    "theme(legend.position = \"right\")\n",
    "\n",
    "# us <- map_data(\"state\")\n",
    "# StateDatabyWeekforMap_2018-19week40-9\n",
    "statedata <- read.csv(\"StateDataforMap_2018-19week4.csv\")\n",
    "statedata$activityval <- as.numeric(gsub(\"Level \",\"\",statedata$ACTIVITY.LEVEL))\n",
    "statedata$state <- tolower(statedata$STATENAME)\n",
    "#ggtitle(\"2018-19 Influenza Season Week 7 ending Feb 16, 2019\")+\n",
    "cdc<-plot_usmap(data = statedata, values = \"activityval\", lines = \"black\") + \n",
    "  scale_fill_continuous(\n",
    "    low = \"green\", high = \"red\", name = \"ILI Activity Level\", label = scales::comma\n",
    "  ) + \n",
    "# labs(title = \"2018-19 Influenza Season Week 4 ending Jan 26, 2019\") + \n",
    "theme(legend.position = \"right\")\n",
    "dev.off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from CDC heat map of Influenza season,Influenza is mostly spread on south-east and central america.Our collected tweets using sneezing keyword heat map also shows it spread mostly on south-east and central america.New York and Florida are the states which mostly shows contradictory results for cdc and our map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong>pdf:</strong> 2"
      ],
      "text/latex": [
       "\\textbf{pdf:} 2"
      ],
      "text/markdown": [
       "**pdf:** 2"
      ],
      "text/plain": [
       "pdf \n",
       "  2 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "png(\"cdc_vs_twitter_sneezing.png\")\n",
    "multiplot(cdc, twitter, cols=2)\n",
    "dev.off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CDC VS Twitter(Flu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [1] connecticut          tennessee            ohio                \n",
      " [4] georgia              pennsylvania         california          \n",
      " [7] new york             tennessee            illinois            \n",
      "[10] indiana              florida              massachusetts       \n",
      "[13] florida              illinois             kentucky            \n",
      "[16] massachusetts        georgia              tennessee           \n",
      "[19] north carolina       minnesota            south carolina      \n",
      "[22] kentucky             tennessee            georgia             \n",
      "[25] florida              pennsylvania         new york            \n",
      "[28] ohio                 kentucky             tennessee           \n",
      "[31] florida              new york             ohio                \n",
      "[34] illinois             ohio                 georgia             \n",
      "[37] district of columbia ohio                \n",
      "40 Levels: alabama arizona arkansas california colorado ... wyoming\n"
     ]
    }
   ],
   "source": [
    "data <- read.csv(\"statesList_flu_keyword.csv\", sep = \",\")\n",
    "data$address=str_remove_all(data$address,\" \");\n",
    "data$address=str_remove_all(gsub('[[:digit:]]+', '', data$address),\",usa\");\n",
    "data$address=substrFromEnd(data$address,2);\n",
    "testPoints <- data.frame(x = tolower(getStateName(data$address)), y = data$lat)\n",
    "print(testPoints$x[!is.na(testPoints$x)][3:40]);\n",
    "# data <- read.csv(\"statesList.csv\", sep = \",\")\n",
    "# testPoints <- data.frame(x = data$lon, y = data$lat)\n",
    "# raw<-latlong2state(testPoints)\n",
    "# states <- raw[!is.na(raw)]\n",
    "states <- testPoints$x[!is.na(testPoints$x)]\n",
    "tweet_count <- as.data.frame(table(states))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong>pdf:</strong> 2"
      ],
      "text/latex": [
       "\\textbf{pdf:} 2"
      ],
      "text/markdown": [
       "**pdf:** 2"
      ],
      "text/plain": [
       "pdf \n",
       "  2 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "png(\"cdc_vs_twitter_flu.png\")\n",
    "us <- map_data(\"state\")                           \n",
    "us <- map_data(\"state\")\n",
    "tweet_count=data.frame(tweet_count)\n",
    "tweet_count$state<-tweet_count$states\n",
    "#ggtitle(\"2018-19 Influenza Season Week 7 ending Feb 16, 2019\")+\n",
    "twitter<-plot_usmap(data = tweet_count, values = \"Freq\", lines = \"black\") + \n",
    "  scale_fill_continuous(\n",
    "    low = \"green\", high = \"red\", name = \"Tweet Count\", label = scales::comma\n",
    "  ) + \n",
    "# labs(title = \"2018-19 Influenza Season Week 4 ending Jan 26, 2019\") + \n",
    "theme(legend.position = \"right\")\n",
    "\n",
    "# us <- map_data(\"state\")\n",
    "# StateDatabyWeekforMap_2018-19week40-9\n",
    "statedata <- read.csv(\"StateDataforMap_2018-19week4.csv\")\n",
    "statedata$activityval <- as.numeric(gsub(\"Level \",\"\",statedata$ACTIVITY.LEVEL))\n",
    "statedata$state <- tolower(statedata$STATENAME)\n",
    "#ggtitle(\"2018-19 Influenza Season Week 7 ending Feb 16, 2019\")+\n",
    "cdc<-plot_usmap(data = statedata, values = \"activityval\", lines = \"black\") + \n",
    "  scale_fill_continuous(\n",
    "    low = \"green\", high = \"red\", name = \"ILI Activity Level\", label = scales::comma\n",
    "  ) + \n",
    "# labs(title = \"2018-19 Influenza Season Week 4 ending Jan 26, 2019\") + \n",
    "theme(legend.position = \"right\")\n",
    "dev.off()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from CDC heat map of Influenza season,Influenza is mostly spread on south-east and central america.Our collected tweets using sneezing keyword heat map also shows it spread mostly on south-east and central america.New York is the state which mostly shows contradictory results for cdc and our map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<strong>pdf:</strong> 2"
      ],
      "text/latex": [
       "\\textbf{pdf:} 2"
      ],
      "text/markdown": [
       "**pdf:** 2"
      ],
      "text/plain": [
       "pdf \n",
       "  2 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "png(\"cdc_vs_twitter_flu.png\")\n",
    "multiplot(cdc, twitter, cols=2)\n",
    "dev.off()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
